Our current workflow for automatic updates:

- Transform union catalogue to RDF in N-Triple serialization, trigger update (lod_hdfs_update.sh):
- Copy N-Triples from local file system to HDFS: @hadoop fs -copyFromLocal *.nt hdfs://10.1.2.111:8020/user/hduser/update/@
- SSH into the Hadoop cluster and start the updates script: @ssh hduser@hadoop-pc1.hbz-nrw.de "cd /home/typhon/scripts; bash -x job_updates.sh"@ (this requires a correct SSH key setup: public key of user logging in should be in hduser's authorized_keys).
- Delete the updates in the local file system: @rm *.nt@

The job_updates.sh script calls the conversion and index scripts:

- Use the external LOD (for enrichment) and our updated LOD as the input: @sh convert-lobid.sh extlod,update output/json-ld-update@
- Index the resulting output in elasticsearch: @sh index.sh output/json-ld-update@
- Delete the updates in HDFS: @hadoop fs -rm update/*@